{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this notebook in Google Colab : [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Riminder/hrflow-cookbook/blob/main/examples/%5BParsing%5D%20profile_job_parsing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2023 HrFlow's AI Research Department\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 HrFlow's AI Research Department. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Google Colaboratory tutorial for developers. **In only 4 steps**, we'll help you **parse, store and get your profiles and jobs**. This will enable you to test the powerful **HrFlow.ai parsing feature**.\n",
    "\n",
    "Before we proceed, please ensure that you have created a source and a board in HrFlow.ai to store your data. You can find detailed instructions on how to create them through the following links:\n",
    "- **Create your source**: [Connectors Source Documentation](https://developers.hrflow.ai/docs/connectors-source)\n",
    "- **Create your board**: [Connectors Board Documentation](https://developers.hrflow.ai/docs/connectors-board)\n",
    "\n",
    "**NB :** It is also necessary to activate the **real-time (Sync)** parsing, please follow the steps described in the links above.\n",
    "\n",
    "Now, let's take a quick look at how this notebook is organized:\n",
    "1. **Profiles**\n",
    "\n",
    "    **1.1 üìù Parse and Store Profiles** \n",
    "    \n",
    "    **1.2 üë∑ Retrieve Stored Profiles** \n",
    "2. **Jobs**\n",
    "    \n",
    "    **2.1 üìù Parse and Store Jobs**\n",
    "    \n",
    "    **2.2 üõ† Retrieve Stored Jobs**\n",
    "\n",
    "Let's get started and harness the capabilities of HrFlow.ai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet hrflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import typing\n",
    "from glob import glob\n",
    "from functools import wraps\n",
    "from getpass import getpass\n",
    "from time import time, sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from pydantic import BaseModel, root_validator\n",
    "\n",
    "from hrflow import Hrflow\n",
    "\n",
    "\n",
    "API_SECRET = getpass(\"YOUR_API_SECRET\")\n",
    "API_USER = getpass(\"USER@EMAIL.DOMAIN\")\n",
    "SOURCE_KEY = getpass(\"YOUR_SOURCE_KEY\")\n",
    "BOARD_KEY = getpass(\"YOUR_BOARD_KEY\")\n",
    "\n",
    "\n",
    "def rate_limiter(\n",
    "    max_requests_per_minute=30,\n",
    "    min_sleep_per_request=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator that applies rate limiting to a function.\n",
    "\n",
    "    Args:\n",
    "        max_requests_per_minute (int): The maximum number of requests allowed per minute.\n",
    "        min_sleep_per_request (float): The minimum time to sleep between consecutive requests.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        requests_per_minute = 0\n",
    "        last_reset_time = time()\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            nonlocal requests_per_minute, last_reset_time\n",
    "\n",
    "            current_time = time()\n",
    "            elapsed_time = current_time - last_reset_time\n",
    "\n",
    "            if elapsed_time < 60:\n",
    "                requests_per_minute += 1\n",
    "                if requests_per_minute >= max_requests_per_minute:\n",
    "                    sleep(60 - elapsed_time)\n",
    "                    requests_per_minute = 0\n",
    "                    last_reset_time = time()\n",
    "            else:\n",
    "                requests_per_minute = 0\n",
    "                last_reset_time = current_time\n",
    "\n",
    "            sleep(min_sleep_per_request)\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "client = Hrflow(api_secret=API_SECRET, api_user=API_USER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Create Folders to Store CVs and Job Descriptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the file structure below. You should store your raw CVs (any format `.pdf`, `.jpg`, `.png`...) in the `raw_cvs` folder and your raw job descriptions (raw texte format `.txt`) in the `raw_jobs` folder.\n",
    "\n",
    "```bash\n",
    ".\n",
    "‚îú‚îÄ‚îÄ raw_cvs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cv_{reference_1}.pdf\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cv_{reference_2}.pdf\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ retrieved_cvs/\n",
    "‚îú‚îÄ‚îÄ raw_jobs/\n",
    "|   ‚îú‚îÄ‚îÄ job_{reference_1}.txt\n",
    "|   ‚îú‚îÄ‚îÄ job_{reference_2}.txt\n",
    "|   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ parsed_jobs/\n",
    "‚îú‚îÄ‚îÄ retrieved_jobs/\n",
    "‚îî‚îÄ‚îÄ this_notebook_üòÅ.ipynb\n",
    "```\n",
    "\n",
    "**NB**: \n",
    "- Please make sure to respect the naming convention for your files : i.e. `cv_{reference}.[extension]` and `job_{reference}.txt`.\n",
    "- The reference of your files should be unique, it will be the main way to identify your profiles and jobs in HrFlow.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"raw_cvs\", \"retrieved_cvs\",\"raw_jobs\", \"parsed_jobs\", \"retrieved_jobs\"]\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Profiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 üìù Parse and Store Profiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse and upload your CVs to HrFlow.ai, we will use the `Profile` class below, it is using the [HrFlow.ai python SDK](https://github.com/Riminder/python-hrflow-api). However, to enrich profiles it is possible to add :\n",
    "\n",
    "- `created_at` to a profile following the `ISO 8601` format, e.g. `2021-01-01T00:00:00`. Otherwise, it will automatically be set to the current time. \n",
    "- `tags` and `metadata` to a profile using the same class. Below is an example of `tags` and `metadata` :\n",
    "\n",
    "```python\n",
    "tags = [\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME\",\n",
    "        \"value\": \"DUMMY_VALUE\"          # <- Could be None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME_2\",\n",
    "        \"value\": \"DUMMY_VALUE_2\"        # <- Could be None\n",
    "    },\n",
    "]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME\",\n",
    "        \"value\": \"DUMMY_VALUE\"          # <- Could be None\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_cv = rate_limiter()(client.profile.parsing.add_file)\n",
    "\n",
    "\n",
    "class Profile(BaseModel):\n",
    "    raw_document_path: str\n",
    "    reference: str\n",
    "    created_at: typing.Optional[str]\n",
    "    tags: typing.Optional[typing.List[typing.Dict]]\n",
    "    metadatas: typing.Optional[typing.List[typing.Dict]]\n",
    "    source_key: str = SOURCE_KEY\n",
    "    profile_json: typing.Optional[typing.Dict[str, typing.Any]] = None\n",
    "\n",
    "    @root_validator\n",
    "    def check_custom_fields(cls, values):\n",
    "        tags = values.get(\"tags\")\n",
    "        metadata = values.get(\"metadatas\")\n",
    "        if tags:\n",
    "            for tag in tags:\n",
    "                assert tag.get(\"name\") and tag.get(\n",
    "                    \"value\"), \"All tags must have a name and a value\"\n",
    "        if metadata:\n",
    "            for data in metadata:\n",
    "                assert data.get(\"name\") and data.get(\n",
    "                    \"value\"), \"All metadata must have a name and a value\"\n",
    "        return values\n",
    "\n",
    "    def parse_document(self):\n",
    "        with open(self.raw_document_path, \"rb\") as file:\n",
    "            profile_file = file.read()\n",
    "        resp = upload_cv(\n",
    "            source_key=self.source_key,\n",
    "            profile_file=profile_file,\n",
    "            profile_content_type=\"application/pdf\",\n",
    "            reference=self.reference,\n",
    "            tags=self.tags,\n",
    "            metadatas=self.metadatas,\n",
    "            created_at=self.created_at,\n",
    "            sync_parsing=1,\n",
    "            sync_parsing_indexing=1,\n",
    "            webhook_parsing_sending=0,\n",
    "        )\n",
    "        self.profile_json = resp[\"data\"][\"profile\"]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_profiles = []\n",
    "for cv_path in tqdm(glob(\"raw_cvs/*\"), desc=\"Parsing CVs\"):\n",
    "    file_name = cv_path.split(\"/\")[-1]\n",
    "    profile_reference = re.findall(r\"cv_(.*)\\.\", file_name)[0]\n",
    "    local_profiles.append(\n",
    "        Profile(\n",
    "            raw_document_path=cv_path,\n",
    "            reference=profile_reference,\n",
    "        ).parse_document()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 üë∑ Retrieve Stored Profiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets retrieve the profiles from the HrFlow.ai store. These profiles will be stored in the `retrieved_cvs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = client.profile.storing.list(source_keys=[SOURCE_KEY])[\"meta\"][\"maxPage\"]\n",
    "retrieved_profiles = []\n",
    "for page in tqdm(range(1, max_page + 1), \"Retrieving profiles\"):\n",
    "    retrieved_profiles += client.profile.storing.list(\n",
    "        source_keys=[SOURCE_KEY], page=page, return_profile=True)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in retrieved_profiles:\n",
    "    with open(f\"retrieved_cvs/profile_{profile['reference']}.json\", \"w\") as file:\n",
    "        json.dump(profile, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 üìù Parse and Store Jobs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse, format and upload a job description to HrFlow.ai, we will use the `Job` class below. First it will parse the texte description using the HrFlow.ai parsing API, then it will format the job description to match the HrFlow.ai format. Finally, it will upload the job to HrFlow.ai. However, to enrich jobs it is possible to add :\n",
    "\n",
    "- Personalized `sections`. The class below doesn't split the job description into multiple sections, If your job description contains multiple sections like : `Responsibilities`, `Requirements`, `Benefits`... you can specify this by editing the `format` method in the `Job` class.\n",
    "- `created_at` to a job following the `ISO 8601` format, e.g. `2021-01-01T00:00:00`. Otherwise, it will automatically be set to the current time. \n",
    "- `tags` and `metadata` to a job using the same class. Below is an example of `tags` and `metadata` :\n",
    "\n",
    "```python\n",
    "tags = [\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME\",\n",
    "        \"value\": \"DUMMY_VALUE\"          # <- Could be None\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME_2\",\n",
    "        \"value\": \"DUMMY_VALUE_2\"        # <- Could be None\n",
    "    },\n",
    "]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"name\": \"DUMMY_NAME\",\n",
    "        \"value\": \"DUMMY_VALUE\"          # <- Could be None\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_job = rate_limiter()(client.text.parsing.post)\n",
    "upload_job = rate_limiter()(client.job.storing.add_json)\n",
    "\n",
    "\n",
    "class Job(BaseModel):\n",
    "    raw_text: str\n",
    "    reference: str\n",
    "    created_at: typing.Optional[str]\n",
    "    tags: typing.Optional[typing.List[typing.Dict]]\n",
    "    metadatas: typing.Optional[typing.List[typing.Dict]]\n",
    "    board_key: str = BOARD_KEY\n",
    "    parsed_job: typing.Optional[typing.Dict] = None\n",
    "    job_json: typing.Optional[typing.Dict] = None\n",
    "\n",
    "    @root_validator\n",
    "    def check_custom_fields(cls, values):\n",
    "        tags = values.get(\"tags\")\n",
    "        metadata = values.get(\"metadatas\")\n",
    "        if tags:\n",
    "            for tag in tags:\n",
    "                assert tag.get(\"name\") and tag.get(\n",
    "                    \"value\"), \"All tags must have a name and a value\"\n",
    "        if metadata:\n",
    "            for data in metadata:\n",
    "                assert data.get(\"name\") and data.get(\n",
    "                    \"value\"), \"All metadata must have a name and a value\"\n",
    "        return values\n",
    "\n",
    "    def parse_job(self):\n",
    "        response = parse_job(text=self.raw_text)\n",
    "        self.parsed_job = response[\"data\"][\"parsing\"]\n",
    "        return self\n",
    "\n",
    "    def format(self):\n",
    "        self.job_json = dict(\n",
    "            name=self.parsed_job.get(\"job_titles\", [\"\"])[0],\n",
    "            reference=self.reference,\n",
    "            url=None,\n",
    "            summary=None,\n",
    "            created_at=self.created_at,\n",
    "            sections=[{\n",
    "                \"name\": \"section 1\",\n",
    "                \"title\": \"title section 1\",\n",
    "                \"description\": self.raw_text,\n",
    "            }],\n",
    "            skills=[\n",
    "                {\n",
    "                    \"name\": skill,\n",
    "                    \"value\": None,\n",
    "                    \"type\": \"hard\"\n",
    "                }\n",
    "                for skill in self.parsed_job.get(\"skills_hard\")\n",
    "            ] + [{\n",
    "                \"name\": skill,\n",
    "                \"value\": None,\n",
    "                \"type\": \"soft\"\n",
    "            }\n",
    "                for skill in self.parsed_job.get(\"skills_soft\")\n",
    "            ],\n",
    "            languages=[{\n",
    "                \"name\": lang,\n",
    "                \"value\": None\n",
    "            }\n",
    "                for lang in self.parsed_job.get(\"languages\")\n",
    "            ],\n",
    "            tasks=[{\n",
    "                \"name\": task,\n",
    "                \"value\": None\n",
    "            }\n",
    "                for task in self.parsed_job.get(\"tasks\")\n",
    "            ],\n",
    "            certifications=[{\n",
    "                \"name\": certification,\n",
    "                \"value\": None\n",
    "            }\n",
    "                for certification in self.parsed_job.get(\"certifications\")\n",
    "            ],\n",
    "        )\n",
    "        if self.tags:\n",
    "            self.job_json[\"tags\"] = self.tags\n",
    "        if self.metadatas:\n",
    "            self.job_json[\"metadatas\"] = self.metadatas\n",
    "\n",
    "        return self\n",
    "\n",
    "    def cache_job(self, folder=\"parsed_jobs\"):\n",
    "        with open(f\"{folder}/{self.reference}.json\", \"w\") as file:\n",
    "            json.dump(self.job_json, file)\n",
    "        return self\n",
    "\n",
    "    def upload_to_board(self):\n",
    "        resp = upload_job(\n",
    "            board_key=self.board_key, job_json=self.job_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since jobs are first parsed, we will first cache them into the `parsed_jobs` folder. Then, we will upload them to HrFlow.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_jobs = []\n",
    "for job in tqdm(glob(\"raw_jobs/*\"), desc=\"Parsing jobs\"):\n",
    "    file_name = job.split(\"/\")[-1]\n",
    "    job_reference = re.findall(r\"job_(.*)\\.\", file_name)[0]\n",
    "    local_jobs.append(\n",
    "        Job(\n",
    "            raw_text=open(job).read(),\n",
    "            reference=job_reference,\n",
    "        ).parse_job().format().cache_job().upload_to_board()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 üõ† Retrieve Stored Jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets retrieve the jobs from the HrFlow.ai store. These jobs will be stored in the `retrieved_jobs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = client.job.storing.list(board_keys=[BOARD_KEY])[\"meta\"][\"maxPage\"]\n",
    "retreived_jobs = []\n",
    "for page in tqdm(range(1, max_page + 1), \"Retrieving jobs\"):\n",
    "    retreived_jobs += client.job.storing.list(board_keys=[BOARD_KEY], page=page, return_job=True)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in retreived_jobs: \n",
    "    with open(f\"retrieved_jobs/job_{job['reference']}.json\", \"w\") as file:\n",
    "        json.dump(job, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re-training-aAT72eTQ-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
